import requests
import pandas as pd
from datetime import datetime
from urllib.parse import urlparse
from collections import defaultdict
from newspaper import Article
from bs4 import BeautifulSoup

API_KEY = ""
SEARCH_TERMS = ["$NVDA", "$AMZN", "$PLTR", "$AMD", "$QQQ", "$VOO", "$SOFI"]
NEWS_LIMIT = 5
OUTPUT_DIR = ""
TRUSTED_DOMAINS = ",".join([
    "wsj.com", "ft.com", "bloomberg.com", "reuters.com",
    "forbes.com", "cnbc.com", "marketwatch.com",
    "businessinsider.com", "economist.com", "seekingalpha.com", "marketbeat.com"
])

def fetch_news(query):
    url = "https://newsapi.org/v2/everything"
    params = {
        "q": query,
        "language": "en",
        "sortBy": "publishedAt",
        "pageSize": NEWS_LIMIT,
        "domains": TRUSTED_DOMAINS,
        "apiKey": API_KEY
    }
    response = requests.get(url, params=params)
    if response.status_code != 200:
        print(f"‚ùå Failed for {query}: HTTP {response.status_code}")
        print(response.text)
        return []
    return response.json().get("articles", [])


def extract_article_text(url):
    try:
        article = Article(url)
        article.download()
        article.parse()
        text = article.text.strip()
        if text:
            return text
    except Exception as e:
        print(f"[newspaper3k] Failed: {url} ‚Üí {e}")

    try:
        headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.content, "html.parser")
        paragraphs = soup.find_all("p")
        text = "\n".join(p.get_text() for p in paragraphs).strip()
        if text:
            return text
    except Exception as e:
        print(f"[BeautifulSoup fallback] Failed: {url} ‚Üí {e}")

    return ""


def save_to_csv(data):
    df = pd.DataFrame(data)
    filename = f"{OUTPUT_DIR}\\trusted_news_{datetime.now():%Y%m%d_%H%M%S}.csv"
    df.to_csv(filename, index=False, encoding="utf-8")
    print(f"\n‚úÖ Saved full data to {filename}")


def main():
    all_news = []
    THRESHOLD = 100

    for term in SEARCH_TERMS:
        print(f"\nüîé Searching '{term}'...")
        articles = fetch_news(term)

        for art in articles:
            print(f"‚Ä¢ {art['title']} ({art['source']['name']})")

            article_text = extract_article_text(art['url'])

            if not article_text or len(article_text.strip()) < THRESHOLD:
                print(
                    f"üõë Skipping article with insufficient content: {art['url']}")
                continue

            all_news.append({
                "query": term,
                "title": art["title"],
                "source": art["source"]["name"],
                "url": art["url"],
                "publishedAt": art["publishedAt"],
                "article_text": article_text
            })

    if all_news:
        save_to_csv(all_news)
    else:
        print("No news to save.")


if __name__ == "__main__":
    main()
